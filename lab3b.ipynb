{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c038c3",
   "metadata": {},
   "source": [
    "# Laboratorio 3 - Data Science Clasificación de rótulos de tráfico utilizando CNN Le-Net\n",
    "Javier Ramírez - 21600  \n",
    "Mario Cristales - 21631"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "52d6ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, f1_score\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6a582",
   "metadata": {},
   "source": [
    "## 1. Preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c0541cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['coords', 'labels', 'features', 'sizes'])\n",
      "Key: coords, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: labels, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: features, Type: <class 'numpy.ndarray'>, Length: 34799\n",
      "Key: sizes, Type: <class 'numpy.ndarray'>, Length: 34799\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo de entrenamiento\n",
    "with open('entrenamiento.p', 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "\n",
    "# Inspeccionar las llaves del diccionario\n",
    "print(data.keys())\n",
    "\n",
    "# Inspeccionar los tipos de datos de las llaves\n",
    "for key in data.keys():\n",
    "    print(f\"Key: {key}, Type: {type(data[key])}, Length: {len(data[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c8fd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento\n",
    "def load_data(pickle_file):\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    return data['features'], data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5534fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento, validación y prueba\n",
    "X_train, y_train = load_data('entrenamiento.p')\n",
    "X_val, y_val = load_data('validacion.p')\n",
    "X_test, y_test = load_data('prueba.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c32b70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar las imágenes\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convertir las etiquetas a arreglos de numpy\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Convertir las etiquetas a one-hot encoding\n",
    "X_train = tf.image.rgb_to_grayscale(X_train)\n",
    "X_val = tf.image.rgb_to_grayscale(X_val)\n",
    "X_test = tf.image.rgb_to_grayscale(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5faba4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (34799, 32, 32, 1), Labels shape: (34799,)\n",
      "Validation data shape: (4410, 32, 32, 1), Labels shape: (4410,)\n",
      "Test data shape: (12630, 32, 32, 1), Labels shape: (12630,)\n"
     ]
    }
   ],
   "source": [
    "# visualizar las dimensiones de los datos\n",
    "print(f'Train data shape: {X_train.shape}, Labels shape: {y_train.shape}')\n",
    "print(f'Validation data shape: {X_val.shape}, Labels shape: {y_val.shape}')\n",
    "print(f'Test data shape: {X_test.shape}, Labels shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a98bfab",
   "metadata": {},
   "source": [
    "## 2. Implementación de la arquitectura Le-Net:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca3c17",
   "metadata": {},
   "source": [
    "### 1. Presentación de la Arquitectura LeNet\n",
    "\n",
    "**LeNet** es una de las arquitecturas pioneras en el ámbito de las Redes Neuronales Convolucionales (CNNs), desarrollada por Yann LeCun y su equipo en 1998. Fue diseñada específicamente para la tarea de reconocimiento de dígitos escritos a mano, como los presentes en el conjunto de datos MNIST. LeNet demostró ser efectiva en la extracción automática de características relevantes a partir de imágenes, reduciendo la necesidad de preprocesamiento manual.\n",
    "\n",
    "- **Conv1**: La primera capa convolucional aplica 6 filtros de 5x5 a las imágenes de entrada de tamaño 32x32 píxeles. Como resultado, se generan 6 mapas de características de 28x28, debido a la reducción en tamaño por los bordes.\n",
    "- **Pool1**: A continuación, se aplica una capa de Max-pooling con ventanas de 2x2 y un stride de 2, lo que reduce las dimensiones de los mapas de características a 14x14. Esta reducción de dimensionalidad ayuda a disminuir la carga computacional y a resumir las características más importantes.\n",
    "- **Conv2**: La segunda capa convolucional aplica 16 filtros de 5x5 a los mapas de características resultantes, generando 16 mapas de características de 10x10. Esta capa profundiza la capacidad de la red para extraer características más complejas y específicas.\n",
    "- **Pool2**: Similar a la primera capa de pooling, se aplica Max-pooling con ventanas de 2x2, reduciendo los mapas de características a 5x5. Este proceso continúa condensando la información relevante.\n",
    "- **FC1**: La salida de la segunda capa de pooling se aplana y se pasa a una capa completamente conectada con 120 neuronas. En esta capa, la red combina las características extraídas para formar representaciones más abstractas.\n",
    "- **FC2**: La siguiente capa completamente conectada tiene 84 neuronas, donde se realiza una mayor combinación y refinamiento de las características.\n",
    "- **Capa de Salida**: Finalmente, la red termina con una capa de salida que contiene 10 neuronas, correspondientes a las 10 clases posibles de dígitos (0-9). Esta capa utiliza softmax para generar probabilidades de clasificación.\n",
    "\n",
    "### 2. Diseño de la Red LeNet\n",
    "El diseño de LeNet sigue un patrón estructurado de capas convolucionales y de pooling, seguidas por capas completamente conectadas. Esta arquitectura permitió que LeNet fuera uno de los primeros modelos en lograr una alta precisión en tareas de reconocimiento visual.\n",
    "![Diagrama de Arquitectura](diagrama.png)\n",
    "\n",
    "### 3. Proceso de Convolución y Pooling\n",
    "\n",
    "- **Convolución**: \n",
    "  - **Extracción de Características Locales**: Los filtros convolucionales, también llamados kernels, actúan como detectores de características. Al deslizarse por la imagen, estos filtros capturan patrones locales como bordes, texturas, y formas simples.\n",
    "  - **Función de Activación (ReLU)**: Tras la convolución, se aplica la función de activación ReLU (Rectified Linear Unit), que introduce no linealidad al modelo. Esta no linealidad es crucial para permitir que la red aprenda representaciones complejas.\n",
    "  \n",
    "- **Pooling**:\n",
    "  - **Max-Pooling**: Este proceso selecciona el valor máximo dentro de una ventana de tamaño fijo, como 2x2. Max-pooling reduce la dimensionalidad de los mapas de características, conservando solo la información más relevante. Esto no solo ayuda a reducir la complejidad computacional, sino que también agrega una forma de invarianza a la traslación, haciendo que la red sea más robusta ante pequeñas variaciones en la posición de las características detectadas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5393db86",
   "metadata": {},
   "source": [
    "## 3. Construcción del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "632eee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\javie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">156</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">400</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">84</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,164</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">43</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,655</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │           \u001b[38;5;34m156\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m6\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m16\u001b[0m)     │         \u001b[38;5;34m2,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m16\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m400\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)            │        \u001b[38;5;34m48,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m84\u001b[0m)             │        \u001b[38;5;34m10,164\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m43\u001b[0m)             │         \u001b[38;5;34m3,655\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,511</span> (252.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m64,511\u001b[0m (252.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">64,511</span> (252.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m64,511\u001b[0m (252.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear el modelo\n",
    "model = models.Sequential()\n",
    "\n",
    "# Capa de convolución 1\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Capa de convolución 2\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(5, 5), activation='relu'))\n",
    "model.add(layers.AveragePooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Aplanar las salidas\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Capa completamente conectada 1\n",
    "model.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "# Capa completamente conectada 2\n",
    "model.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(layers.Dense(units=43, activation='softmax'))\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192d020",
   "metadata": {},
   "source": [
    "La **función de pérdida** mide qué tan bien el modelo está realizando la tarea para la que fue entrenado, comparando las predicciones con las etiquetas verdaderas. Su objetivo es cuantificar el error del modelo, permitiendo ajustes durante el entrenamiento para mejorar su precisión.\n",
    "\n",
    "El **optimizador** es un algoritmo que ajusta los parámetros del modelo para minimizar la función de pérdida. Utiliza el gradiente descendente y otras técnicas para encontrar los valores óptimos de los pesos y sesgos del modelo, mejorando así su rendimiento en la tarea específica.\n",
    "\n",
    "Ambos son cruciales: la función de pérdida guía el entrenamiento al proporcionar una medida de error, mientras que el optimizador ajusta los parámetros del modelo para reducir ese error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9983e74",
   "metadata": {},
   "source": [
    "## 4.Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132565c",
   "metadata": {},
   "source": [
    "Durante el entrenamiento de una red neuronal, se alimentan datos de entrada a la red y se calculan las predicciones. La función de pérdida evalúa el error entre las predicciones y las etiquetas verdaderas, y el optimizador ajusta los pesos del modelo para minimizar este error a través de múltiples iteraciones, mejorando así el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e6f1246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "# Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b8c424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8da47287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.1924 - loss: 3.0613 - val_accuracy: 0.6365 - val_loss: 1.2809\n",
      "Epoch 2/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.7319 - loss: 0.9314 - val_accuracy: 0.7492 - val_loss: 0.8521\n",
      "Epoch 3/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8480 - loss: 0.5458 - val_accuracy: 0.7884 - val_loss: 0.7421\n",
      "Epoch 4/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8945 - loss: 0.3997 - val_accuracy: 0.8231 - val_loss: 0.6396\n",
      "Epoch 5/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9164 - loss: 0.3158 - val_accuracy: 0.8288 - val_loss: 0.6249\n",
      "Epoch 6/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9341 - loss: 0.2495 - val_accuracy: 0.8379 - val_loss: 0.6399\n",
      "Epoch 7/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9450 - loss: 0.2071 - val_accuracy: 0.8481 - val_loss: 0.6285\n",
      "Epoch 8/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9538 - loss: 0.1798 - val_accuracy: 0.8440 - val_loss: 0.6029\n",
      "Epoch 9/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9607 - loss: 0.1469 - val_accuracy: 0.8635 - val_loss: 0.5821\n",
      "Epoch 10/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9697 - loss: 0.1206 - val_accuracy: 0.8619 - val_loss: 0.6486\n",
      "Epoch 11/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9713 - loss: 0.1088 - val_accuracy: 0.8590 - val_loss: 0.7265\n",
      "Epoch 12/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9748 - loss: 0.0925 - val_accuracy: 0.8551 - val_loss: 0.6097\n",
      "Epoch 13/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9767 - loss: 0.0884 - val_accuracy: 0.8653 - val_loss: 0.6870\n",
      "Epoch 14/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9826 - loss: 0.0650 - val_accuracy: 0.8658 - val_loss: 0.7283\n",
      "Epoch 15/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9831 - loss: 0.0647 - val_accuracy: 0.8719 - val_loss: 0.6641\n",
      "Epoch 16/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9830 - loss: 0.0635 - val_accuracy: 0.8694 - val_loss: 0.6560\n",
      "Epoch 17/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9848 - loss: 0.0562 - val_accuracy: 0.8753 - val_loss: 0.7608\n",
      "Epoch 18/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9877 - loss: 0.0460 - val_accuracy: 0.8844 - val_loss: 0.7048\n",
      "Epoch 19/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9882 - loss: 0.0429 - val_accuracy: 0.8714 - val_loss: 0.7355\n",
      "Epoch 20/20\n",
      "\u001b[1m272/272\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9903 - loss: 0.0355 - val_accuracy: 0.8805 - val_loss: 0.7344\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, \n",
    "                    validation_data=validation_dataset, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c98e450",
   "metadata": {},
   "source": [
    "## 5.Evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "778e44f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Precisión: 0.89\n",
      "F1-Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "print(f'Precisión: {precision:.2f}')\n",
    "print(f'F1-Score: {f1:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "43d4b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m395/395\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Matriz de Confusión:\n",
      "[[ 31  29   0 ...   0   0   0]\n",
      " [  3 690   4 ...   8   0   0]\n",
      " [  0  67 648 ...   0   0   0]\n",
      " ...\n",
      " [  0   6   0 ...  42   0   0]\n",
      " [  0   0   0 ...   0  42   4]\n",
      " [  0   0   0 ...   0   0  86]]\n",
      "\n",
      "Reporte de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.52      0.51        60\n",
      "           1       0.77      0.96      0.85       720\n",
      "           2       0.92      0.86      0.89       750\n",
      "           3       0.80      0.92      0.86       450\n",
      "           4       0.91      0.85      0.88       660\n",
      "           5       0.89      0.87      0.88       630\n",
      "           6       0.95      0.79      0.87       150\n",
      "           7       0.85      0.90      0.87       450\n",
      "           8       0.88      0.79      0.83       450\n",
      "           9       0.97      0.92      0.94       480\n",
      "          10       0.95      0.97      0.96       660\n",
      "          11       0.89      0.92      0.90       420\n",
      "          12       0.96      0.94      0.95       690\n",
      "          13       0.98      0.99      0.98       720\n",
      "          14       0.92      0.90      0.91       270\n",
      "          15       0.96      0.94      0.95       210\n",
      "          16       0.93      0.98      0.95       150\n",
      "          17       1.00      0.91      0.95       360\n",
      "          18       0.94      0.75      0.83       390\n",
      "          19       0.63      0.53      0.58        60\n",
      "          20       0.64      0.78      0.70        90\n",
      "          21       0.98      0.49      0.65        90\n",
      "          22       0.98      0.78      0.87       120\n",
      "          23       0.47      0.86      0.61       150\n",
      "          24       0.53      0.52      0.53        90\n",
      "          25       0.96      0.83      0.89       480\n",
      "          26       0.82      0.82      0.82       180\n",
      "          27       0.90      0.45      0.60        60\n",
      "          28       0.90      0.63      0.75       150\n",
      "          29       0.70      0.82      0.76        90\n",
      "          30       0.65      0.65      0.65       150\n",
      "          31       0.80      0.95      0.87       270\n",
      "          32       0.59      0.95      0.73        60\n",
      "          33       0.91      0.98      0.94       210\n",
      "          34       0.88      0.96      0.92       120\n",
      "          35       0.98      0.90      0.94       390\n",
      "          36       0.98      0.96      0.97       120\n",
      "          37       0.95      0.92      0.93        60\n",
      "          38       0.91      0.93      0.92       690\n",
      "          39       0.83      0.93      0.88        90\n",
      "          40       0.61      0.47      0.53        90\n",
      "          41       1.00      0.70      0.82        60\n",
      "          42       0.88      0.96      0.91        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.85      0.82      0.83     12630\n",
      "weighted avg       0.89      0.88      0.88     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hacer predicciones con el modelo\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convertir las predicciones a clases\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "\n",
    "# Generar el reporte de clasificación\n",
    "class_report = classification_report(y_test, y_pred_classes)\n",
    "\n",
    "# Presentar los resultados\n",
    "print(\"Matriz de Confusión:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf7c4a",
   "metadata": {},
   "source": [
    "### Interpretación \n",
    "\n",
    "- **Historial de Entrenamiento**:\n",
    "  - **Pérdida (`loss`)**: Disminuye constantemente, indicando que el modelo está mejorando en los datos de entrenamiento.\n",
    "  - **Precisión (`accuracy`)**: Aumenta significativamente, mostrando que el modelo está haciendo mejores predicciones.\n",
    "\n",
    "- **Métricas Finales**:\n",
    "  - **Precisión Global**: 0.89, lo que significa que el 89% de las predicciones son correctas.\n",
    "  - **F1-Score**: 0.88, reflejando un buen equilibrio entre precisión y recall.\n",
    "\n",
    "- **Matriz de Confusión**:\n",
    "  - Muestra que el modelo es eficaz en la clasificación de la mayoría de las clases, aunque hay algunas clases con mayor confusión.\n",
    "\n",
    "En general, el modelo está funcionando bien, con alta precisión y un buen equilibrio en las métricas de rendimiento.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
